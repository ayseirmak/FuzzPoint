#include <stdio.h>
#include <assert.h>

// Declaration of the entry function
void entry(const float tensor_input[1][8], float tensor_output[1][4]);

int main() {
    // Initialize all input and output tensors
    float tensor_input[1][8] = {
        {-0.630, 0.120, 0.020, -0.010, 0.050, -0.040, 1.000, 1.000}
    };
    float tensor_output[1][4];

    // Execute the process
    entry(tensor_input, tensor_output);

    // Check the assertion condition with the provided deterministic data
    // Note: Tensor output values should be corrected with actual expected results to verify correctly
    assert(!(tensor_output[0][2] <= tensor_output[0][3]));

    printf("Assertion passed! The output values met the expected condition.\n");
    return 0;
}

// Include the functions generated by the ONNX model converter here
// For readability, I include the function headers used in the process

/* 
    Function definitions from the ONNX model
    You can place the functions such as node_Flatten_0, 
    node_Gemm_1, node_Relu_2, node_Gemm_3, node_Relu_4, 
    node_Gemm_5 here from your converted model logic 
    (as they are quite long, they are not directly included in this illustrative snippet)
*/

// Flattening function
static inline void node_Flatten_0( const float tensor_input[1][8], float tensor_7[1][8] ) {
    float *input = (float*)tensor_input;
    float *output = (float*)tensor_7;
    for( uint32_t i=0; i<8; i++ )
        output[i] = input[i];
}

// Other function definitions
// node_Gemm_1, node_Relu_2, node_Gemm_3, node_Relu_4, node_Gemm_5

union tensor_union_0 {
    float tensor_7[1][8];
    float tensor_9[1][64];
    float tensor_11[1][64];
};
static union tensor_union_0 tu0;

union tensor_union_1 {
    float tensor_8[1][64];
    float tensor_10[1][64];
};
static union tensor_union_1 tu1;

// Final entry function after processing inputs through the network
void entry(const float tensor_input[1][8], float tensor_output[1][4]) {
    node_Flatten_0( tensor_input, tu0.tensor_7);
    node_Gemm_1( tu0.tensor_7, tensor_q_net_0_weight, tensor_q_net_0_bias, tu1.tensor_8);
    node_Relu_2( tu1.tensor_8, tu0.tensor_9);
    node_Gemm_3( tu0.tensor_9, tensor_q_net_2_weight, tensor_q_net_2_bias, tu1.tensor_10);
    node_Relu_4( tu1.tensor_10, tu0.tensor_11);
    node_Gemm_5( tu0.tensor_11, tensor_q_net_4_weight, tensor_q_net_4_bias, tensor_output);
}